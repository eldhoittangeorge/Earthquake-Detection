{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import preprocessor as p\r\n",
    "import wordninja\r\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "data = pd.read_csv('../../Data/new_data.csv',index_col=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# Converting to lowercase\r\n",
    "def to_lower(text):\r\n",
    "    return text.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# Removing punctuations\r\n",
    "punctuation = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'\r\n",
    "def remove_punctuations(text):\r\n",
    "    # text = re.sub(\".+\",\"\",text)\r\n",
    "    return re.sub('[%s]' % re.escape(punctuation),'',text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "stop_words = set(stopwords.words('english'))\r\n",
    "stop_words.add('http')\r\n",
    "def remove_stopwords(text):\r\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Rephrasing url, mentions, and retweets\r\n",
    "def rephrase_text(text):\r\n",
    "    p.set_options(p.OPT.URL,p.OPT.MENTION)\r\n",
    "    parsed_text = p.tokenize(text)\r\n",
    "    clean_text = re.sub(\"RT\\s\\$MENTION\\$:\",\"retweet\",parsed_text)\r\n",
    "    clean_text = re.sub(\"\\$MENTION\\$:\",\"mentions\",clean_text)\r\n",
    "    clean_text = re.sub(\"\\$URL\\$\",\"link\",clean_text)\r\n",
    "    return clean_text\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "# Expanding hashtags\r\n",
    "def handle_hashtag(text):\r\n",
    "    p.set_options(p.OPT.HASHTAG) \r\n",
    "    parsed_text = p.parse(text)\r\n",
    "    if(not parsed_text.hashtags):\r\n",
    "        return text\r\n",
    "    for hashtag in parsed_text.hashtags:\r\n",
    "        match = hashtag.match\r\n",
    "        text = re.sub(match, \" \".join(wordninja.split(match)), text)\r\n",
    "    return text\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "# Removing emojis\r\n",
    "def remove_emoji(text):\r\n",
    "    p.set_options(p.OPT.EMOJI,p.OPT.SMILEY)\r\n",
    "    tokenized_text = p.tokenize(text)\r\n",
    "    tokenized_text = re.sub(re.escape(\"$EMOJI$\"),\"\",tokenized_text)\r\n",
    "    return tokenized_text\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# Lemmatizing words\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "def lemmatize_word(text):\r\n",
    "    return lemmatizer.lemmatize(text)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# Removing whitespace\r\n",
    "def remove_whitespace(text):\r\n",
    "    return re.sub(' +',\" \",text)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "def preprocessing_pipline(text):\r\n",
    "    text = remove_stopwords(text)\r\n",
    "    text = rephrase_text(text)\r\n",
    "    text = handle_hashtag(text)\r\n",
    "    text = remove_emoji(text)\r\n",
    "    text = lemmatize_word(text)\r\n",
    "    text = remove_whitespace(text)\r\n",
    "    text = to_lower(text)\r\n",
    "    text = remove_punctuations(text)\r\n",
    "    return text\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "data[\"Tweet Text\"] = data[\"Tweet Text\"].map(preprocessing_pipline)\r\n",
    "data.to_csv(\"../../Data/clean_data.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ds': conda)"
  },
  "interpreter": {
   "hash": "b5d4274ebb9180454c804b5ee419348493c013ff5d68b712df2e1ca9accf6ce7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}